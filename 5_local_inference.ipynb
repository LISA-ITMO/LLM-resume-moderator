{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Wav2Vec2ForCTC, Wav2Vec2Processor, TextStreamer\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rools = '\\n'.join(map(str, json.load(open('data/resume_rools.json'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "rools = '\\n'.join(map(str, rools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHUNKS_LEN = 2048\n",
    "OVERLAP_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_MODEL_NAME = \"t-tech/T-lite-it-1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(INFERENCE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c81fcce5a884d00a6847f88ebfcc05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    INFERENCE_MODEL_NAME, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",\n",
    "    max_memory={0: '10GB', 1: '10GB', 2: '10GB'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Ты — ИИ-модератор резюме. Проверь текст резюме на соответствие следующим правилам. Если есть нарушения, верни JSON-объект с типом нарушения и фрагментом текста, который нарушает правило. Если нарушений нет, верни статус \"OK\".\n",
    "\n",
    "**Правила:** \n",
    "{rools}\n",
    "\n",
    "**Инструкции:**\n",
    "\n",
    "- Тщательно проанализируй каждое предложение в резюме.\n",
    "- Если нарушений нет, верни \"status\": \"OK\" и пустой массив violated_rules.\n",
    "- Если фрагмент нарушает правило, укажи его точную цитату в resume_fragment.\n",
    "- Ответ должен содержать твои рассуждения по каждому правилу оканчивающийся вердиктом, затем должен быть результат в формате JSON, без Markdown и комментариев.\n",
    "\n",
    "Формат ответа: **Рассуждения:**, **Результат:** { \"status\": \"OK\" | \"violation\", \"violated_rules\": [] | [ { \"id\": \"rule_id\", \"condition\": \"Текст условия правила на русском\", \"resume_fragment\": \"Точная цитата из резюме\" } ] }\n",
    "\n",
    "**Текст резюме:**\n",
    "{resume_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_with_overlap(text: str, tokenizer: AutoTokenizer, max_length: int, overlap: int) -> List[str]:\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + max_length\n",
    "        chunks.append(tokens[start:end])\n",
    "        start = end - overlap\n",
    "        \n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(response_str: str) -> dict:\n",
    "    parts = response_str.split(\"**Результат:**\")\n",
    "    \n",
    "    reasoning = parts[0].replace(\"**Рассуждения:**\\n\", \"\").strip()\n",
    "\n",
    "    json_match = re.search(r'```json\\n(.*?)\\n```', parts[1], re.DOTALL)\n",
    "    json_str = json_match.group(1) if json_match else '{}'\n",
    "\n",
    "    try:\n",
    "        result_dict = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        result_dict = {\"error\": \"Invalid JSON format\"}\n",
    "    \n",
    "    return {\n",
    "        \"reasoning\": reasoning,\n",
    "        \"result\": result_dict\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(resume_text: str) -> Dict[str, str]:\n",
    "    \n",
    "    text_chunks = split_text_with_overlap(\n",
    "        resume_text, \n",
    "        tokenizer,\n",
    "        max_length=2048,\n",
    "        overlap=128\n",
    "    )\n",
    "    \n",
    "    all_reasoning = []\n",
    "    all_violations = []\n",
    "    overall_status = \"OK\"\n",
    "    \n",
    "    for chunk in text_chunks:\n",
    "        formatted_prompt = prompt.replace('{rools}', rools).replace('{resume_text}', chunk)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Твоя задача - анализировать части резюме.\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "        \n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(inference_model.device)\n",
    "        \n",
    "        generated_ids = inference_model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "            temperature=None\n",
    "        )\n",
    "        \n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] \n",
    "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        answer_content = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        parsed = parse_answer(answer_content)\n",
    "        \n",
    "        all_reasoning.append(parsed['reasoning'])\n",
    "        if parsed['result']['status'] == 'violation':\n",
    "            overall_status = 'violation'\n",
    "            all_violations.extend(parsed['result']['violated_rules'])\n",
    "    \n",
    "    return {\n",
    "        'reasoning': \"\\n\\n\".join(all_reasoning),\n",
    "        'result': {\n",
    "            'status': overall_status,\n",
    "            'violated_rules': all_violations\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/processed_resume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_llm_resp = get_answer(df.text_data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': '1. **rule_1**: Резюме содержит информацию о трудовой деятельности, что соответствует правилу, так как указана должность и период работы.\\n2. **rule_2**: В резюме указана должность \"водитель\", которая является реальной профессией, поэтому правило соблюдается.\\n3. **rule_4**: Резюме написано на русском языке, что соответствует правилу.\\n4. **rule_5**: Информация о работодателе содержит только имя и фамилию, что не является нарушением, так как это стандартная практика для ИП.\\n5. **rule_6**: Все информация в резюме относится к соискателю, что соответствует правилу.\\n6. **rule_7**: В резюме отсутствуют маты и токсичные выражения, что соответствует правилу.',\n",
       " 'result': {'status': 'OK', 'violated_rules': []}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_llm_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/905 [07:55<7:49:44, 31.67s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['t_pro_answer'] = df['text_data'].progress_apply(get_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 23:18:12,735 - INFO - Resuming from index 10\n",
      "2025-02-22 23:18:35,163 - INFO - Checkpoint saved at index 10\n",
      "  1%|▏         | 12/905 [00:57<7:26:36, 30.01s/it]"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def process_dataframe(df, checkpoint_interval=20):\n",
    "    last_checkpoint = get_last_checkpoint()\n",
    "    start_index = 0\n",
    "    results = []\n",
    "    \n",
    "    if last_checkpoint:\n",
    "        start_index = last_checkpoint['index'] + 1\n",
    "        results = last_checkpoint['results']\n",
    "        logging.info(f\"Resuming from index {start_index}\")\n",
    "\n",
    "    try:\n",
    "        for i in tqdm(range(start_index, len(df)), initial=start_index, total=len(df)):\n",
    "            try:\n",
    "                result = get_answer(df.text_data[i])\n",
    "                results.append(result)\n",
    "                \n",
    "                if (i - start_index) % checkpoint_interval == 0:\n",
    "                    save_checkpoint(i, results)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing row {i}: {str(e)}\")\n",
    "                results.append({'error': str(e)})\n",
    "                \n",
    "        save_checkpoint(len(df)-1, results)\n",
    "        return pd.DataFrame(results)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Process interrupted. Saving last checkpoint...\")\n",
    "        save_checkpoint(i, results)\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "def save_checkpoint(index, results):\n",
    "    checkpoint = {\n",
    "        'timestamp': datetime.now(),\n",
    "        'index': index,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    with open('checkpoint.pkl', 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "        \n",
    "    logging.info(f\"Checkpoint saved at index {index}\")\n",
    "\n",
    "def get_last_checkpoint():\n",
    "    if os.path.exists('checkpoint.pkl'):\n",
    "        try:\n",
    "            with open('checkpoint.pkl', 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading checkpoint: {str(e)}\")\n",
    "\n",
    "\n",
    "result_df = process_dataframe(df)\n",
    "result_df.to_csv('final_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t_pro_answer'] = result_df['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t_pro_status'] = result_df['result'].apply(lambda el: el['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t_pro_reasoning'] = result_df['reasoning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t_pro_status\n",
       "OK           828\n",
       "violation     77\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['t_pro_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198    violation\n",
       "242    violation\n",
       "425    violation\n",
       "428    violation\n",
       "822    violation\n",
       "Name: t_pro_status, dtype: object"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Результат'] == 'Отклонено', 't_pro_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t_pro_status\n",
       "OK           690\n",
       "violation     59\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Результат'] == 'Принято', 't_pro_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative = (df.loc[df['Результат'] == 'Отклонено', 't_pro_status'] == 'OK').astype(int).sum()\n",
    "false_positive = (df.loc[df['Результат'] == 'Принято', 't_pro_status'] == 'violation').astype(int).sum()\n",
    "recall = (df.loc[df['Результат'] == 'Отклонено', 't_pro_status'] == 'violation').astype(int).mean()\n",
    "precision = (df.loc[df['Результат'] == 'Принято', 't_pro_status'] == 'OK').astype(int).mean()\n",
    "f1_score = 2 * (recall * precision) / (recall + precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false negative: 0\n",
      "false positive: 0\n",
      "recall: 1.00\n",
      "precision: 0.92\n",
      "f1-score: 0.96\n"
     ]
    }
   ],
   "source": [
    "print(f'false negative: {false_negative}')\n",
    "print(f'false positive: {false_negative}')\n",
    "print(f'recall: {recall:.2f}')\n",
    "print(f'precision: {precision:.2f}')\n",
    "print(f'f1-score: {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\t\u001b[32mnew file:   .ipynb_checkpoints/3_extract_rools-checkpoint.ipynb\u001b[m\n",
      "\t\u001b[32mnew file:   .ipynb_checkpoints/5_local_inference-checkpoint.ipynb\u001b[m\n",
      "\t\u001b[32mmodified:   3_extract_rools.ipynb\u001b[m\n",
      "\t\u001b[32mmodified:   5_local_inference.ipynb\u001b[m\n",
      "\t\u001b[32mnew file:   checkpoint.pkl\u001b[m\n",
      "\t\u001b[32mmodified:   data/resume_rools.json\u001b[m\n",
      "\t\u001b[32mnew file:   final_results.csv\u001b[m\n",
      "\t\u001b[32mnew file:   processing.log\u001b[m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
